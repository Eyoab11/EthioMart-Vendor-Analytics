{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0b53a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Model Interpretability and Inference Test\n",
    "\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# --- 1. Load your fine-tuned model from Google Drive ---\n",
    "# Make sure this path points to the model you just saved.\n",
    "# We'll use the mBERT model since it was the last one we trained.\n",
    "SAVED_MODEL_PATH = os.path.join(GDRIVE_PROJECT_PATH, 'models', 'mbert-cased-ner-finetuned')\n",
    "\n",
    "# Check if the saved model directory exists\n",
    "if not os.path.exists(SAVED_MODEL_PATH):\n",
    "    print(f\"❌ ERROR: Saved model not found at '{SAVED_MODEL_PATH}'\")\n",
    "    print(\"Please make sure you have successfully completed the training and saving step.\")\n",
    "else:\n",
    "    print(f\"--- Loading fine-tuned model from: {SAVED_MODEL_PATH} ---\")\n",
    "    # Load the model into a token-classification pipeline for easy inference\n",
    "    ner_pipeline = pipeline(\n",
    "        \"token-classification\",\n",
    "        model=SAVED_MODEL_PATH,\n",
    "        tokenizer=SAVED_MODEL_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210c9bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. Define a sample sentence to test ---\n",
    "    # Let's use a sentence that we know has entities.\n",
    "    # From your labeled data: \"Laorentou monk strap leather size 39,42,43,44 Price 3500 birr\"\n",
    "    test_sentence_1 = \"Laorentou monk strap leather size 39,42,43,44 Price 3500 birr\"\n",
    "    \n",
    "    # Another example with Amharic\n",
    "    test_sentence_2 = \"saachi የሚይዘው መጠን 3ሊትር ዋጋ 3999 ብር ነው\" # saachi holder size 3liter price 3999 birr\n",
    "\n",
    "    print(\"\\n--- Running Inference on Sample Sentence 1 ---\")\n",
    "    predictions_1 = ner_pipeline(test_sentence_1)\n",
    "    print(f\"Sentence: '{test_sentence_1}'\")\n",
    "    print(\"Predictions:\")\n",
    "    for entity in predictions_1:\n",
    "        print(f\"  - Word: {entity['word']}, Entity: {entity['entity']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Running Inference on Sample Sentence 2 ---\")\n",
    "    predictions_2 = ner_pipeline(test_sentence_2)\n",
    "    print(f\"Sentence: '{test_sentence_2}'\")\n",
    "    print(\"Predictions:\")\n",
    "    for entity in predictions_2:\n",
    "        print(f\"  - Word: {entity['word']}, Entity: {entity['entity']}, Score: {entity['score']:.4f}\")\n",
    "\n",
    "    # --- 3. Analysis of the Results ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Interpretability Analysis ---\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"As expected from the 0.0 F1-score during training, the model is not successfully identifying our target entities (PRODUCT, PRICE, LOC).\")\n",
    "    print(\"The predictions above likely show 'O' (represented as LABEL_0) for most or all tokens.\")\n",
    "    print(\"This confirms our key finding: the model has learned that predicting 'O' for everything is the safest way to minimize its loss, a classic sign that it has not been trained on enough labeled data to learn the specific patterns of our entities.\")\n",
    "    print(\"\\nThis result, while poor in performance, provides a clear direction for future work: the primary focus must be on increasing the size of the high-quality, manually labeled dataset.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
