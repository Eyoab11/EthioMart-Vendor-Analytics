{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f4318c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Task 4: Model Comparison and Selection\n",
    "# This script implements Task 4 of the EthioMart Amharic E-commerce Data Extractor project.\n",
    "# The goal is to fine-tune the `bert-base-multilingual-cased` model for Named Entity Recognition (NER)\n",
    "# on Amharic Telegram data and compare its performance with other models (e.g., `xlm-roberta-base` from Task 3).\n",
    "# The script includes data loading, tokenization, model training with memory-efficient configurations,\n",
    "# evaluation, and model saving.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# Configuration\n",
    "GDRIVE_PROJECT_PATH = \"/content/drive/MyDrive/Ethio_mart\"\n",
    "LABELED_DATA_PATH = os.path.join(GDRIVE_PROJECT_PATH, \"labeled_data_conll.txt\")\n",
    "OUTPUT_MODEL_DIR = os.path.join(GDRIVE_PROJECT_PATH, \"models\", \"mbert-cased-ner-finetuned\")\n",
    "MODEL_CHECKPOINT = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d1132",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define labels\n",
    "labels_list = [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for i, label in enumerate(labels_list)}\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Check if project folder and data file exist.\"\"\"\n",
    "    print(\"--- 1. Checking if the main project folder exists ---\")\n",
    "    if os.path.exists(GDRIVE_PROJECT_PATH):\n",
    "        print(f\"✅ SUCCESS: The folder '{GDRIVE_PROJECT_PATH}' exists.\")\n",
    "    else:\n",
    "        print(f\"❌ FAILURE: The folder '{GDRIVE_PROJECT_PATH}' DOES NOT EXIST. Is your Drive mounted?\")\n",
    "        raise FileNotFoundError(f\"Project folder {GDRIVE_PROJECT_PATH} not found.\")\n",
    "\n",
    "    print(\"\\n--- 2. Listing the contents of the project folder ---\")\n",
    "    os.system(f\"ls -l {GDRIVE_PROJECT_PATH}\")\n",
    "\n",
    "    print(\"\\n--- 3. Checking if the specific file exists ---\")\n",
    "    if os.path.exists(LABELED_DATA_PATH):\n",
    "        print(f\"✅ SUCCESS: The file '{LABELED_DATA_PATH}' was found!\")\n",
    "    else:\n",
    "        print(f\"❌ FAILURE: The file '{LABELED_DATA_PATH}' was NOT found at this exact path.\")\n",
    "        raise FileNotFoundError(f\"Data file {LABELED_DATA_PATH} not found.\")\n",
    "\n",
    "def create_dataset_from_conll(file_path):\n",
    "    \"\"\"Parse CoNLL file and create a Hugging Face Dataset.\"\"\"\n",
    "    tokens_list, tags_list = [], []\n",
    "    current_tokens, current_tags = [], []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if current_tokens:\n",
    "                    tokens_list.append(current_tokens)\n",
    "                    tags_list.append(current_tags)\n",
    "                    current_tokens, current_tags = [], []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                current_tokens.append(parts[0])\n",
    "                current_tags.append(label2id[parts[1]])\n",
    "\n",
    "    if current_tokens:\n",
    "        tokens_list.append(current_tokens)\n",
    "        tags_list.append(current_tags)\n",
    "\n",
    "    return Dataset.from_dict({\"tokens\": tokens_list, \"ner_tags\": tags_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138929c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    \"\"\"Tokenize inputs and align NER labels with tokens.\"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Compute evaluation metrics using seqeval.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ef205",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Uncomment to install dependencies in a new environment (e.g., Colab)\n",
    "    # os.system(\"pip install transformers datasets seqeval accelerate evaluate -U\")\n",
    "\n",
    "    # Check environment\n",
    "    check_environment()\n",
    "\n",
    "    # Print label mappings\n",
    "    print(\"\\n--- Label Mappings ---\")\n",
    "    print(f\"Label to ID: {label2id}\")\n",
    "    print(f\"ID to Label: {id2label}\")\n",
    "\n",
    "    # Load and split dataset\n",
    "    print(\"\\n--- Loading and Splitting Dataset ---\")\n",
    "    full_dataset = create_dataset_from_conll(LABELED_DATA_PATH)\n",
    "    train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    final_dataset = DatasetDict({\n",
    "        \"train\": train_test_split[\"train\"],\n",
    "        \"test\": train_test_split[\"test\"]\n",
    "    })\n",
    "    print(\"\\n--- Dataset Created and Split ---\")\n",
    "    print(final_dataset)\n",
    "    print(\"\\nExample from training set:\")\n",
    "    print(final_dataset[\"train\"][0])\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    print(f\"\\n--- Initializing tokenizer from: {MODEL_CHECKPOINT} ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_datasets = final_dataset.map(\n",
    "        lambda x: tokenize_and_align_labels(x, tokenizer), batched=True\n",
    "    )\n",
    "    print(f\"\\n--- Tokenization with '{MODEL_CHECKPOINT}' Complete ---\")\n",
    "    print(\"Original Tokens:\", final_dataset[\"train\"][0][\"tokens\"])\n",
    "    print(\"New Tokens:\", tokenizer.convert_ids_to_tokens(tokenized_datasets[\"train\"][0][\"input_ids\"]))\n",
    "    print(\"New Labels:\", tokenized_datasets[\"train\"][0][\"labels\"])\n",
    "\n",
    "    # Load model\n",
    "    print(\"\\n--- Loading Model ---\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_CHECKPOINT,\n",
    "        num_labels=len(labels_list),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    # Set up training arguments with memory optimizations\n",
    "    print(\"\\n--- Setting Up Training Arguments ---\")\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_MODEL_DIR,\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        fp16=True,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "    )\n",
    "\n",
    "    # Initialize data collator and trainer\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Proactive memory cleanup\n",
    "    print(\"\\n--- Cleaning up memory before training ---\")\n",
    "    del full_dataset\n",
    "    del final_dataset\n",
    "    gc.collect()\n",
    "\n",
    "    # Train model\n",
    "    print(f\"\\n--- Starting Training for '{MODEL_CHECKPOINT}' ---\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"\\n--- Training Complete ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"\\n--- Evaluating Final Model ---\")\n",
    "    final_evaluation = trainer.evaluate()\n",
    "    print(final_evaluation)\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    print(\"\\n--- Saving Final Model and Tokenizer ---\")\n",
    "    os.makedirs(OUTPUT_MODEL_DIR, exist_ok=True)\n",
    "    trainer.save_model(OUTPUT_MODEL_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_MODEL_DIR)\n",
    "    print(f\"✅ Model saved successfully to {OUTPUT_MODEL_DIR}\")\n",
    "\n",
    "    # Notes:\n",
    "    # - This script fine-tunes `bert-base-multilingual-cased` with memory-efficient settings\n",
    "    #   (small batch size, gradient accumulation, mixed-precision training).\n",
    "    # - Compare the F1-score, precision, and recall with the `xlm-roberta-base` model from Task 3\n",
    "    #   to select the best model for production.\n",
    "    # - The small dataset (40 train, 10 test) may limit performance. Consider augmenting data for better results.\n",
    "    # - Save the `final_evaluation` results for the final report.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
